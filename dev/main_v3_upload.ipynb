{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [ENG/KOR] Preprocessing & Feature Selection/Engineering & Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_selection import mutual_info_regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions we will use.\n",
    "\n",
    "사용할 함수들을 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It will rapidly score the train data at each process.\n",
    "def score_dataset(df, target, numertic_only=True):\n",
    "    X = df.drop(target, axis=1)\n",
    "    y = df[target]\n",
    "\n",
    "    model = XGBRegressor(\n",
    "        max_depth=3,\n",
    "        n_estimators=70,\n",
    "        eval_metric='rmsle',\n",
    "        random_state=1\n",
    "    )\n",
    "\n",
    "    if numertic_only:\n",
    "        X = X.select_dtypes(exclude=[\"category\", \"object\"])\n",
    "    else:\n",
    "        # Label encoding for categoricals\n",
    "        for colname in X.select_dtypes([\"category\", \"object\"]):\n",
    "            X[colname], _ = X[colname].factorize()\n",
    "    \n",
    "    score = cross_val_score(\n",
    "        model, X, y, cv=3, scoring=\"neg_mean_absolute_error\",\n",
    "    )\n",
    "\n",
    "    score = -1 * score.mean()\n",
    "    score = np.sqrt(score)\n",
    "    return score\n",
    "\n",
    "\n",
    "# Get a ratio of missing values at each column.\n",
    "def get_missing_raio(df, threshold=0):\n",
    "    n_rows = df.shape[0]\n",
    "\n",
    "    n_missing_values = df.isna().sum()\n",
    "    ratio_missing_values = n_missing_values / n_rows\n",
    "    ratio_missing_values = ratio_missing_values[ratio_missing_values >= threshold]\n",
    "    \n",
    "    return ratio_missing_values\n",
    "\n",
    "\n",
    "# Delete columns that have many missing values.\n",
    "def delete_columns(df, threshold, target=None):\n",
    "    ratio_missing_values = get_missing_raio(df)\n",
    "\n",
    "    over_threshold_columns = list(ratio_missing_values[ratio_missing_values > threshold].index)\n",
    "\n",
    "    if target and target in over_threshold_columns:\n",
    "        over_threshold_columns.remove(target)\n",
    "\n",
    "    df = df.drop(over_threshold_columns, axis=1)\n",
    "\n",
    "    return df, over_threshold_columns\n",
    "\n",
    "\n",
    "# Impute missing values.\n",
    "def impute_missing_values(df, method, value=0):\n",
    "    basic_methods = ['fill', 'mean', 'median', 'value']\n",
    "    \n",
    "    df_numeric = df.select_dtypes(exclude=['O'])\n",
    "    df_object = df.select_dtypes(include=['O'])\n",
    "\n",
    "    tool = None\n",
    "    if method in basic_methods:\n",
    "        if method == 'mean':\n",
    "            mean = df_numeric.mean()\n",
    "            df_numeric.fillna(mean, inplace=True)\n",
    "            tool = mean\n",
    "        elif method == 'median':\n",
    "            median = df_numeric.median()\n",
    "            df_numeric.fillna(median, inplace=True)\n",
    "            tool = median\n",
    "        elif method == 'value':\n",
    "            df_numeric.fillna(value, inplace=True)\n",
    "            tool = value\n",
    "        else:\n",
    "            df_numeric.fillna(method='ffill', inplace=True)\n",
    "            df_numeric.fillna(method='bfill', inplace=True)\n",
    "    \n",
    "    elif method == 'linear':\n",
    "        df_numeric = df_numeric.interpolate(method=method, limit_direction='both')\n",
    "    else:\n",
    "        raise Exception(\"Not a valid method.\")\n",
    "\n",
    "\n",
    "    df_columns = list(df.columns)\n",
    "    final_df = pd.concat([df_numeric, df_object], axis=1)\n",
    "    final_df = final_df[df_columns]\n",
    "\n",
    "    return final_df, tool\n",
    "\n",
    "\n",
    "# Get the mutual information scores.\n",
    "def make_mi_scores(X, y, discrete_features):\n",
    "    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features)\n",
    "    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n",
    "    mi_scores = mi_scores.sort_values(ascending=False)\n",
    "    return mi_scores\n",
    "\n",
    "\n",
    "# Delete columns by mutual information score and correlation coefficient.\n",
    "def mi_score(df, target, threshold, corr=False, corr_threshold=None, protected=None):\n",
    "    if not protected:\n",
    "        protected = []\n",
    "        \n",
    "    df_mi = df.copy()\n",
    "    \n",
    "    X = df_mi.drop(target, axis=1)\n",
    "    y = df_mi.pop(target)\n",
    "    \n",
    "    X_numeric = X.select_dtypes(exclude=['O'])\n",
    "    object_columns = list(X.select_dtypes(include=['O']).columns)\n",
    "    \n",
    "    discrete_features = X_numeric.dtypes == int\n",
    "\n",
    "    mi_scores = make_mi_scores(X_numeric, y, discrete_features)\n",
    "\n",
    "    mi_selected_columns = list(mi_scores.loc[mi_scores >= threshold].index)\n",
    "\n",
    "\n",
    "    # If there are features that have a correlation coefficient \n",
    "    # with the target feature over the 'corr_threshold' argument, \n",
    "    # they won't be deleted.\n",
    "    corr_selected_columns = []\n",
    "    if corr:\n",
    "        df_numeric = df.select_dtypes(exclude=['O'])\n",
    "        df_corr = df_numeric.corr()\n",
    "\n",
    "        corr = abs(df_corr[target])\n",
    "        corr_selected_columns = list(corr[corr >= corr_threshold].index)\n",
    "\n",
    "    selected_columns_all = list(set(mi_selected_columns + corr_selected_columns + protected)) + object_columns\n",
    "    df_selected = df.loc[:, selected_columns_all]\n",
    "\n",
    "    return df_selected, selected_columns_all, mi_scores\n",
    "\n",
    "\n",
    "# One-hot encoding\n",
    "def one_hot(df, target, encoder=None):\n",
    "    if not encoder:\n",
    "        encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "        encoder.fit(df[[target]])\n",
    "\n",
    "    df[target] = df[target].astype('O')\n",
    "    \n",
    "    df_encoded = pd.DataFrame(encoder.transform(df[[target]]))\n",
    "    df_encoded.index = df.index\n",
    "    df_encoded.columns = encoder.get_feature_names_out()\n",
    "    df = df.drop(target, axis=1)\n",
    "\n",
    "    df = pd.concat([df, df_encoded], axis=1)\n",
    "\n",
    "    return df, encoder\n",
    "\n",
    "\n",
    "# A function when we make the 'season' column.\n",
    "SEASON = {\n",
    "    'summer': [12, 1, 2],\n",
    "    'fall': [3, 4, 5],\n",
    "    'winter': [6, 7, 8],\n",
    "    'spring': [9, 10, 11],\n",
    "}\n",
    "\n",
    "def insert_season(x, season=SEASON):\n",
    "    if x in season['summer']:\n",
    "        return 'summer'\n",
    "    elif x in season['fall']:\n",
    "        return 'fall'\n",
    "    elif x in season['winter']:\n",
    "        return 'winter'\n",
    "    elif x in season['spring']:\n",
    "        return 'spring'\n",
    "    else:\n",
    "        raise Exception('unknown week')\n",
    "\n",
    "\n",
    "# Negative values to zero.\n",
    "def neg_to_zero(x):\n",
    "    if x < 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_LAT_LON_YEAR_WEEK</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID_-0.510_29.290_2022_00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID_-0.510_29.290_2022_01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID_-0.510_29.290_2022_02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ID_-0.510_29.290_2022_03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ID_-0.510_29.290_2022_04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24348</th>\n",
       "      <td>ID_-3.299_30.301_2022_44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24349</th>\n",
       "      <td>ID_-3.299_30.301_2022_45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24350</th>\n",
       "      <td>ID_-3.299_30.301_2022_46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24351</th>\n",
       "      <td>ID_-3.299_30.301_2022_47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24352</th>\n",
       "      <td>ID_-3.299_30.301_2022_48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24353 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID_LAT_LON_YEAR_WEEK\n",
       "0      ID_-0.510_29.290_2022_00\n",
       "1      ID_-0.510_29.290_2022_01\n",
       "2      ID_-0.510_29.290_2022_02\n",
       "3      ID_-0.510_29.290_2022_03\n",
       "4      ID_-0.510_29.290_2022_04\n",
       "...                         ...\n",
       "24348  ID_-3.299_30.301_2022_44\n",
       "24349  ID_-3.299_30.301_2022_45\n",
       "24350  ID_-3.299_30.301_2022_46\n",
       "24351  ID_-3.299_30.301_2022_47\n",
       "24352  ID_-3.299_30.301_2022_48\n",
       "\n",
       "[24353 rows x 1 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.copy().loc[:, ['ID_LAT_LON_YEAR_WEEK']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape: (79023, 76)\n",
      "test shape: (24353, 75)\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = './data'\n",
    "\n",
    "train = pd.read_csv(os.path.join(DATA_PATH, 'train.csv'))\n",
    "test = pd.read_csv(os.path.join(DATA_PATH, 'test.csv'))\n",
    "\n",
    "sub = test.copy().loc[:, ['ID_LAT_LON_YEAR_WEEK']]\n",
    "\n",
    "print(f'train shape: {train.shape}')\n",
    "print(f'test shape: {test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_LAT_LON_YEAR_WEEK</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>year</th>\n",
       "      <th>week_no</th>\n",
       "      <th>SulphurDioxide_SO2_column_number_density</th>\n",
       "      <th>SulphurDioxide_SO2_column_number_density_amf</th>\n",
       "      <th>SulphurDioxide_SO2_slant_column_number_density</th>\n",
       "      <th>SulphurDioxide_cloud_fraction</th>\n",
       "      <th>SulphurDioxide_sensor_azimuth_angle</th>\n",
       "      <th>...</th>\n",
       "      <th>Cloud_cloud_top_height</th>\n",
       "      <th>Cloud_cloud_base_pressure</th>\n",
       "      <th>Cloud_cloud_base_height</th>\n",
       "      <th>Cloud_cloud_optical_depth</th>\n",
       "      <th>Cloud_surface_albedo</th>\n",
       "      <th>Cloud_sensor_azimuth_angle</th>\n",
       "      <th>Cloud_sensor_zenith_angle</th>\n",
       "      <th>Cloud_solar_azimuth_angle</th>\n",
       "      <th>Cloud_solar_zenith_angle</th>\n",
       "      <th>emission</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID_-0.510_29.290_2019_00</td>\n",
       "      <td>-0.51</td>\n",
       "      <td>29.29</td>\n",
       "      <td>2019</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.000108</td>\n",
       "      <td>0.603019</td>\n",
       "      <td>-0.000065</td>\n",
       "      <td>0.255668</td>\n",
       "      <td>-98.593887</td>\n",
       "      <td>...</td>\n",
       "      <td>3664.436218</td>\n",
       "      <td>61085.809570</td>\n",
       "      <td>2615.120483</td>\n",
       "      <td>15.568533</td>\n",
       "      <td>0.272292</td>\n",
       "      <td>-12.628986</td>\n",
       "      <td>35.632416</td>\n",
       "      <td>-138.786423</td>\n",
       "      <td>30.752140</td>\n",
       "      <td>3.750994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID_-0.510_29.290_2019_01</td>\n",
       "      <td>-0.51</td>\n",
       "      <td>29.29</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.728214</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.130988</td>\n",
       "      <td>16.592861</td>\n",
       "      <td>...</td>\n",
       "      <td>3651.190311</td>\n",
       "      <td>66969.478735</td>\n",
       "      <td>3174.572424</td>\n",
       "      <td>8.690601</td>\n",
       "      <td>0.256830</td>\n",
       "      <td>30.359375</td>\n",
       "      <td>39.557633</td>\n",
       "      <td>-145.183930</td>\n",
       "      <td>27.251779</td>\n",
       "      <td>4.025176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID_-0.510_29.290_2019_02</td>\n",
       "      <td>-0.51</td>\n",
       "      <td>29.29</td>\n",
       "      <td>2019</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000514</td>\n",
       "      <td>0.748199</td>\n",
       "      <td>0.000385</td>\n",
       "      <td>0.110018</td>\n",
       "      <td>72.795837</td>\n",
       "      <td>...</td>\n",
       "      <td>4216.986492</td>\n",
       "      <td>60068.894448</td>\n",
       "      <td>3516.282669</td>\n",
       "      <td>21.103410</td>\n",
       "      <td>0.251101</td>\n",
       "      <td>15.377883</td>\n",
       "      <td>30.401823</td>\n",
       "      <td>-142.519545</td>\n",
       "      <td>26.193296</td>\n",
       "      <td>4.231381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ID_-0.510_29.290_2019_03</td>\n",
       "      <td>-0.51</td>\n",
       "      <td>29.29</td>\n",
       "      <td>2019</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>5228.507736</td>\n",
       "      <td>51064.547339</td>\n",
       "      <td>4180.973322</td>\n",
       "      <td>15.386899</td>\n",
       "      <td>0.262043</td>\n",
       "      <td>-11.293399</td>\n",
       "      <td>24.380357</td>\n",
       "      <td>-132.665828</td>\n",
       "      <td>28.829155</td>\n",
       "      <td>4.305286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ID_-0.510_29.290_2019_04</td>\n",
       "      <td>-0.51</td>\n",
       "      <td>29.29</td>\n",
       "      <td>2019</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.000079</td>\n",
       "      <td>0.676296</td>\n",
       "      <td>-0.000048</td>\n",
       "      <td>0.121164</td>\n",
       "      <td>4.121269</td>\n",
       "      <td>...</td>\n",
       "      <td>3980.598120</td>\n",
       "      <td>63751.125781</td>\n",
       "      <td>3355.710107</td>\n",
       "      <td>8.114694</td>\n",
       "      <td>0.235847</td>\n",
       "      <td>38.532263</td>\n",
       "      <td>37.392979</td>\n",
       "      <td>-141.509805</td>\n",
       "      <td>22.204612</td>\n",
       "      <td>4.347317</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 76 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ID_LAT_LON_YEAR_WEEK  latitude  longitude  year  week_no  \\\n",
       "0  ID_-0.510_29.290_2019_00     -0.51      29.29  2019        0   \n",
       "1  ID_-0.510_29.290_2019_01     -0.51      29.29  2019        1   \n",
       "2  ID_-0.510_29.290_2019_02     -0.51      29.29  2019        2   \n",
       "3  ID_-0.510_29.290_2019_03     -0.51      29.29  2019        3   \n",
       "4  ID_-0.510_29.290_2019_04     -0.51      29.29  2019        4   \n",
       "\n",
       "   SulphurDioxide_SO2_column_number_density  \\\n",
       "0                                 -0.000108   \n",
       "1                                  0.000021   \n",
       "2                                  0.000514   \n",
       "3                                       NaN   \n",
       "4                                 -0.000079   \n",
       "\n",
       "   SulphurDioxide_SO2_column_number_density_amf  \\\n",
       "0                                      0.603019   \n",
       "1                                      0.728214   \n",
       "2                                      0.748199   \n",
       "3                                           NaN   \n",
       "4                                      0.676296   \n",
       "\n",
       "   SulphurDioxide_SO2_slant_column_number_density  \\\n",
       "0                                       -0.000065   \n",
       "1                                        0.000014   \n",
       "2                                        0.000385   \n",
       "3                                             NaN   \n",
       "4                                       -0.000048   \n",
       "\n",
       "   SulphurDioxide_cloud_fraction  SulphurDioxide_sensor_azimuth_angle  ...  \\\n",
       "0                       0.255668                           -98.593887  ...   \n",
       "1                       0.130988                            16.592861  ...   \n",
       "2                       0.110018                            72.795837  ...   \n",
       "3                            NaN                                  NaN  ...   \n",
       "4                       0.121164                             4.121269  ...   \n",
       "\n",
       "   Cloud_cloud_top_height  Cloud_cloud_base_pressure  Cloud_cloud_base_height  \\\n",
       "0             3664.436218               61085.809570              2615.120483   \n",
       "1             3651.190311               66969.478735              3174.572424   \n",
       "2             4216.986492               60068.894448              3516.282669   \n",
       "3             5228.507736               51064.547339              4180.973322   \n",
       "4             3980.598120               63751.125781              3355.710107   \n",
       "\n",
       "   Cloud_cloud_optical_depth  Cloud_surface_albedo  \\\n",
       "0                  15.568533              0.272292   \n",
       "1                   8.690601              0.256830   \n",
       "2                  21.103410              0.251101   \n",
       "3                  15.386899              0.262043   \n",
       "4                   8.114694              0.235847   \n",
       "\n",
       "   Cloud_sensor_azimuth_angle  Cloud_sensor_zenith_angle  \\\n",
       "0                  -12.628986                  35.632416   \n",
       "1                   30.359375                  39.557633   \n",
       "2                   15.377883                  30.401823   \n",
       "3                  -11.293399                  24.380357   \n",
       "4                   38.532263                  37.392979   \n",
       "\n",
       "   Cloud_solar_azimuth_angle  Cloud_solar_zenith_angle  emission  \n",
       "0                -138.786423                 30.752140  3.750994  \n",
       "1                -145.183930                 27.251779  4.025176  \n",
       "2                -142.519545                 26.193296  4.231381  \n",
       "3                -132.665828                 28.829155  4.305286  \n",
       "4                -141.509805                 22.204612  4.347317  \n",
       "\n",
       "[5 rows x 76 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check some rows.\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Handling Missing Values\n",
    "\n",
    "결측값을 처리합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will not delete these columns because they are important.\n",
    "# 중요한 칼럼들이기 때문에 이 칼럼들은 삭제하지 않습니다.\n",
    "protected_columns = ['latitude', 'longitude', 'week_no', 'year']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's delete columns that have lots of missing values.\n",
    "\n",
    "We have to set threshold value as an argument.\n",
    "\n",
    "결측값이 많은 칼럼을 제거합니다.\n",
    "\n",
    "문턱값을 정해야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of deleted columns: 7\n"
     ]
    }
   ],
   "source": [
    "train_deleted, deleted_columns = delete_columns(train, 0.3, target='emission')\n",
    "print(f'The number of deleted columns: {len(deleted_columns)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply this process to test data.\n",
    "test_deleted = test.drop(deleted_columns, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will find the ideal method to impute missing values.\n",
    "\n",
    "가장 이상적인 결측값 대체법을 찾습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method \"mean\" completed\n",
      "method \"linear\" completed\n",
      "method \"fill\" completed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['mean', 8.328042073427394],\n",
       " ['linear', 8.361643976634587],\n",
       " ['fill', 8.34957829303958]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "methods = ['mean', 'linear', 'fill']\n",
    "results = []\n",
    "\n",
    "for method in methods:\n",
    "    train_imputed, _ = impute_missing_values(train_deleted, method)\n",
    "    score = score_dataset(train_imputed, 'emission')\n",
    "    results.append([method, score])\n",
    "    print(f'method \"{method}\" completed')\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best method: ['mean', 8.328042073427394]\n"
     ]
    }
   ],
   "source": [
    "best_method = sorted(results, key=lambda x: x[1])[0]\n",
    "print(f'best method: {best_method}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imputed, _ = impute_missing_values(train_deleted, best_method[0])\n",
    "test_imputed, _ = impute_missing_values(test_deleted, best_method[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_now = train_imputed\n",
    "test_now = test_imputed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make some columns: 'month_no', 'covid'\n",
    "\n",
    "This process is copied from [BASSEM GOUTY's code]('https://www.kaggle.com/code/bassemgouty/ps3e20-ensembling-with-score-nudge'). Thank you!\n",
    "\n",
    "\n",
    "'month_no' 칼럼과 'covid' 칼럼을 추가합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_now['date'] = pd.to_datetime('2021' + train_now['week_no'].astype(str) + '0', format='%Y%W%w')\n",
    "train_now['month_no'] = train_now['date'].dt.month\n",
    "train_now.drop(columns=['date'], inplace=True)\n",
    "\n",
    "train_now['covid'] = (train_now.year == 2020) & (train_now.month_no > 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_now['date'] = pd.to_datetime('2021' + test_now['week_no'].astype(str) + '0', format='%Y%W%w')\n",
    "test_now['month_no'] = test_now['date'].dt.month\n",
    "test_now.drop(columns=['date'], inplace=True)\n",
    "\n",
    "test_now['covid'] = (test_now.year == 2020) & (test_now.month_no > 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute one-hot encoding to the 'covid' column.\n",
    "\n",
    "'covid' 칼럼에 one-hot 인코딩을 수행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\loven\\dev\\kaggle_competition\\carbon\\carbon\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:972: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_now, encoder = one_hot(train_now, 'covid')\n",
    "test_now, _ = one_hot(test_now, 'covid', encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add new columns to 'protected_columns'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "protected_columns.extend(['covid_False', 'covid_True', 'month_no'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the 'season' column.\n",
    "\n",
    "'season' 칼럼을 만듭니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "season\n",
      "summer    20874\n",
      "fall      19383\n",
      "winter    19383\n",
      "spring    19383\n",
      "Name: count, dtype: int64 season\n",
      "fall      6461\n",
      "winter    6461\n",
      "spring    6461\n",
      "summer    4970\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "train_now['season'] = train_now['month_no'].apply(insert_season, args=[SEASON])\n",
    "\n",
    "test_now['season'] = test_now['month_no'].apply(insert_season, args=[SEASON])\n",
    "\n",
    "print(train_now['season'].value_counts(), test_now['season'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will split datasets into each season.\n",
    "\n",
    "**Based on these datasets, we will also make models for each season.**\n",
    "\n",
    "데이터셋을 계절별로 나눕니다. 각 데이터셋에 기반하여 모델도 따로 만들 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_summer = train_now.loc[train_now['season'] == 'summer']\n",
    "train_fall = train_now.loc[train_now['season'] == 'fall']\n",
    "train_winter = train_now.loc[train_now['season'] == 'winter']\n",
    "train_spring = train_now.loc[train_now['season'] == 'spring']\n",
    "\n",
    "test_summer = test_now.loc[test_now['season'] == 'summer']\n",
    "test_fall = test_now.loc[test_now['season'] == 'fall']\n",
    "test_winter = test_now.loc[test_now['season'] == 'winter']\n",
    "test_spring = test_now.loc[test_now['season'] == 'spring']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sets = [train_summer, train_fall, train_winter, train_spring]\n",
    "test_sets = [test_summer, test_fall, test_winter, test_spring]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the 'season' columns.\n",
    "for i in range(len(train_sets)):\n",
    "    train_sets[i] = train_sets[i].drop('season', axis=1)\n",
    "\n",
    "for i in range(len(test_sets)):\n",
    "    test_sets[i] = test_sets[i].drop('season', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Selection(by Mutual Information, Correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only features with a MI score(correlation->correlation coefficient) above the thresholds will be selected.\n",
    "\n",
    "문턱값을 넘는 MI 스코어(또는 상관계수)를 갖는 features만 선택합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of columns of 1th data: 9\n",
      "The number of columns of 2th data: 9\n",
      "The number of columns of 3th data: 9\n",
      "The number of columns of 4th data: 10\n"
     ]
    }
   ],
   "source": [
    "selected_columns = []\n",
    "\n",
    "for idx, train in enumerate(train_sets):\n",
    "    df, columns, _ = mi_score(train, 'emission', 0.1, corr=True, corr_threshold=0.1, protected=protected_columns)\n",
    "    columns.remove('emission')\n",
    "    selected_columns.append(columns)\n",
    "    train_sets[idx] = df\n",
    "    print(f'The number of columns of {idx+1}th data: {len(train_sets[idx].columns)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually, this work seems meaningless. As you can see from other participants' codes, Many features, except geographic features, are challenging to use because it is hard to find some close relationship with emission.\n",
    "\n",
    "사실 이 작업은 큰 의미가 없게 보입니다. 지리적 features 외 다른 features들과 emission의 관계를 찾기가 어려워 이들을 사용하기 어렵기 때문입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply this process to test data.\n",
    "for idx, test in enumerate(test_sets):\n",
    "    test_sets[idx] = test.loc[:, selected_columns[idx]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. The end of making datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's synchronize column order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_columns = []\n",
    "\n",
    "for idx, train in enumerate(train_sets):\n",
    "    col = list(map(str, list(train.columns)))\n",
    "    train.columns = col\n",
    "    col.sort()\n",
    "    train_sets[idx] = train[col]\n",
    "    col.remove('emission')\n",
    "    train_columns.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, test in enumerate(test_sets):\n",
    "    col = list(map(str, list(test.columns)))\n",
    "    test.columns = col\n",
    "    test_sets[idx] = test[train_columns[idx]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Modeling(XGBoost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1th model completed\n",
      "2th model completed\n",
      "3th model completed\n",
      "4th model completed\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\"max_depth\":    [8, 10],\n",
    "              \"n_estimators\": [100, 300],\n",
    "              }\n",
    "\n",
    "models = []\n",
    "\n",
    "for idx, train in enumerate(train_sets):\n",
    "    X_train = train.drop(['emission', 'ID_LAT_LON_YEAR_WEEK'], axis=1)\n",
    "    y_train = train['emission']\n",
    "\n",
    "    regressor = xgb.XGBRegressor(eval_metric='rmsle',\n",
    "                                # tree_method='gpu_hist'\n",
    "                                 )\n",
    "\n",
    "    search = GridSearchCV(regressor, param_grid, cv=5).fit(X_train, y_train)\n",
    "\n",
    "    regressor=xgb.XGBRegressor(\n",
    "        n_estimators = search.best_params_[\"n_estimators\"],\n",
    "        max_depth    = search.best_params_[\"max_depth\"],\n",
    "        eval_metric  = 'rmsle',\n",
    "        # tree_method  = 'gpu_hist'\n",
    "        )\n",
    "\n",
    "    regressor.fit(X_train, y_train)\n",
    "\n",
    "    models.append(regressor)\n",
    "\n",
    "    print(f'{idx+1}th model completed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "\n",
    "for idx, test in enumerate(test_sets):\n",
    "    X_test = test.drop('ID_LAT_LON_YEAR_WEEK', axis=1)\n",
    "    prediction = models[idx].predict(X_test)\n",
    "    predictions.append(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, pred in enumerate(predictions):\n",
    "    test_sets[idx].loc[:, 'emission'] = pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_all = pd.concat(test_sets)\n",
    "sub = sub.merge(test_all, on='ID_LAT_LON_YEAR_WEEK').loc[:, ['ID_LAT_LON_YEAR_WEEK', 'emission']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change negative values to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub['emission'] = sub['emission'].apply(neg_to_zero)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will multiply **1.09738621** by each value.\n",
    "I got this multiple by calculating annual CO2 emissions in Rwanda from 2009 to 2018.\n",
    "\n",
    "The CO2 data is [here](https://ourworldindata.org/co2-emissions#global-co2-emissions-from-fossil-fuels-and-land-use-change)\n",
    "\n",
    "각 값에 1.09738621를 곱해줍니다. 해당 값은 르완다의 2009년부터 2018녀까지 연간 CO2 배출량 데이터를 바탕으로 추출한 값입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_LAT_LON_YEAR_WEEK</th>\n",
       "      <th>emission</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID_-0.510_29.290_2022_00</td>\n",
       "      <td>3.359954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID_-0.510_29.290_2022_01</td>\n",
       "      <td>4.508047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID_-0.510_29.290_2022_02</td>\n",
       "      <td>4.404407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ID_-0.510_29.290_2022_03</td>\n",
       "      <td>4.255953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ID_-0.510_29.290_2022_04</td>\n",
       "      <td>4.312966</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ID_LAT_LON_YEAR_WEEK  emission\n",
       "0  ID_-0.510_29.290_2022_00  3.359954\n",
       "1  ID_-0.510_29.290_2022_01  4.508047\n",
       "2  ID_-0.510_29.290_2022_02  4.404407\n",
       "3  ID_-0.510_29.290_2022_03  4.255953\n",
       "4  ID_-0.510_29.290_2022_04  4.312966"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ANNUAL_INCREASEMENT_RATIO = 1.09738621\n",
    "\n",
    "sub['emission'] = sub['emission'] * ANNUAL_INCREASEMENT_RATIO\n",
    "\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv('sub.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "carbon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
